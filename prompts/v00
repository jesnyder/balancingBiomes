import csv
import json
import time
import requests
from pathlib import Path

# ============================================================
# Configuration
# ============================================================

SPECIES_FILE = Path("user_provided/species_list.csv")
OUTPUT_DIR = Path("results")
OUTPUT_DIR.mkdir(parents=True, exist_ok=True)

JSON_OUT = OUTPUT_DIR / "species_detailed.json"
CSV_OUT = OUTPUT_DIR / "species_detailed.csv"

GBIF_API = "https://api.gbif.org/v1/species/match"
NCBI_API = "https://api.ncbi.nlm.nih.gov/taxonomy/v0/taxon/name/{}"

HEADERS = {"User-Agent": "detail-species-script/1.0"}

# ============================================================
# Utilities
# ============================================================

def load_species_list():
    """Load, deduplicate, and alphabetize species list."""
    with open(SPECIES_FILE, newline="", encoding="utf-8") as f:
        species = [row[0].strip() for row in csv.reader(f) if row]
    return sorted(set(species))


def query_gbif(species):
    try:
        r = requests.get(
            GBIF_API,
            params={"name": species},
            headers=HEADERS,
            timeout=20
        )
        return r.json() if r.status_code == 200 else None
    except Exception:
        return None


def query_ncbi(species):
    try:
        url = NCBI_API.format(species.replace(" ", "%20"))
        r = requests.get(url, headers=HEADERS, timeout=20)
        return r.json() if r.status_code == 200 else None
    except Exception:
        return None


def classify_organism(gbif_data):
    if not gbif_data:
        return "unknown"

    kingdom = str(gbif_data.get("kingdom", "")).lower()

    if "plantae" in kingdom:
        return "plant"
    if "bacteria" in kingdom:
        return "bacteria"
    if "archaea" in kingdom:
        return "archaea"
    if "animalia" in kingdom:
        return "animal"

    return "unknown"


def infer_halotolerance(gbif_data, ncbi_data):
    """
    Return:
      - 'yes' if strong halophile indicators exist
      - 'no' if confidently non-halophilic
      - 'unknown' if insufficient evidence
    """
    keywords = ["halophile", "halophilic", "saline", "salt"]

    found = False

    for source in (gbif_data, ncbi_data):
        if not source:
            continue

        text = json.dumps(source).lower()
        if any(k in text for k in keywords):
            found = True

    if found:
        return "yes"

    if gbif_data or ncbi_data:
        return "unknown"

    return "unknown"


def save_outputs(records):
    # JSON (full structure)
    with open(JSON_OUT, "w", encoding="utf-8") as f:
        json.dump(records, f, indent=2)

    # CSV (summary only)
    with open(CSV_OUT, "w", newline="", encoding="utf-8") as f:
        writer = csv.writer(f)
        writer.writerow([
            "species",
            "classification",
            "halotolerant",
            "url"
        ])

        for r in records:
            writer.writerow([
                r["species"],
                r["classification"],
                r["halotolerant"],
                r.get("url", "")
            ])


# ============================================================
# Main Function
# ============================================================

def detail_species():
    species_list = load_species_list()
    print(f"Loaded {len(species_list)} unique species")

    results = []

    for idx, species in enumerate(species_list, 1):
        print(f"[{idx}/{len(species_list)}] Processing: {species}")

        gbif_data = query_gbif(species)
        ncbi_data = query_ncbi(species)

        classification = classify_organism(gbif_data)
        halotolerant = infer_halotolerance(gbif_data, ncbi_data)

        url = None
        if gbif_data and gbif_data.get("usageKey"):
            url = f"https://www.gbif.org/species/{gbif_data['usageKey']}"

        record = {
            "species": species,
            "classification": classification,
            "halotolerant": halotolerant,
            "url": url,
            "gbif": gbif_data,
            "ncbi": ncbi_data
        }

        results.append(record)

        # Save after every species
        save_outputs(results)

        time.sleep(0.3)  # polite API usage

    print("âœ… Species detailing complete.")


if __name__ == "__main__":
    detail_species()
